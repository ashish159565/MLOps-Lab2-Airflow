# 🚀 MLOps Lab 2 – Building an Automated ML Pipeline using Apache Airflow (with Docker)

Hi! I’m **Ashish Gajjela**, and this is my submission for **MLOps Lab 2**, where I built a **fully automated machine learning pipeline** using **Apache Airflow** running inside **Docker**.

This project shows how to orchestrate an end-to-end ML workflow — from **data extraction** all the way to **model deployment** — using modular DAGs that trigger one another automatically.

---

## 🧩 What this project does

The pipeline is divided into **five DAGs**, each handling one stage of the ML lifecycle.  
Here’s how the data flows through them:

```
master_trigger_dag → dag_1_extract → dag_2_transform → dag_3_train → dag_4_deploy
```

### 🧠 Step-by-step explanation:

1. **dag_1_extract**  
   - Downloads the **Iris dataset** from a public source.  
   - Saves it in the `working_data/raw` folder as `iris.csv`.

2. **dag_2_transform**  
   - Reads the raw dataset, performs preprocessing (cleaning, encoding, scaling, etc.).  
   - Outputs a processed version to `working_data/processed/iris_clean.csv`.

3. **dag_3_train**  
   - Loads the processed data.  
   - Trains a **Logistic Regression** model using `scikit-learn`.  
   - Saves the trained model as `working_data/models/model.pkl`.

4. **dag_4_deploy**  
   - Simulates deployment by loading the model and running predictions.  
   - Exports results to `working_data/output/predictions.csv`.

5. **master_trigger_dag**  
   - Orchestrates everything.  
   - When triggered, it automatically runs all other DAGs in order.

---

## 🧠 What I used

- **Apache Airflow** (v2.5.1) with **CeleryExecutor**  
- **Docker Compose** (to containerize everything)
- **PostgreSQL** (metadata database)
- **Redis** (task broker)
- **Python 3.7+**
- **Libraries:** `pandas`, `scikit-learn`, `requests`, `joblib`

---

## 📁 Project structure

```
MLOps-Lab2-Airflow/
│
├── dags/
│   ├── dag_1_extract.py
│   ├── dag_2_transform.py
│   ├── dag_3_train.py
│   ├── dag_4_deploy.py
│   └── master_trigger_dag.py
│
├── working_data/
│   ├── raw/
│   ├── cleanedd/
│   └── model/
│
├── docker-compose.yaml
├── logs/
├── plugins/
└── README.md
```

---

## ⚙️ How to set it up (Step-by-Step)

### 1️⃣ Clone the repository
```bash
git clone https://github.com/ashish159565/MLOps-Lab2-Airflow
cd MLOps-Lab2-Airflow
```

### 2️⃣ Create the `.env` file
Inside your project folder, create a `.env` file with the following:

```bash
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=.
_AIRFLOW_WWW_USER_USERNAME=airflow2
_AIRFLOW_WWW_USER_PASSWORD=airflow2
```

### 3️⃣ Set up permissions
Airflow runs inside the container as user ID 50000, so give it permission to write data:

```bash
sudo mkdir -p working_data/raw working_data/processed working_data/models
sudo chown -R 50000:0 working_data
sudo chmod -R 775 working_data
```

### 4️⃣ Make sure DAGs auto-enable
Open `docker-compose.yaml` and change:
```yaml
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
```

This ensures all DAGs are active automatically.

### 5️⃣ Initialize and start Airflow
Now build and run everything:

```bash
sudo docker compose down --volumes --remove-orphans
sudo docker compose up airflow-init
sudo docker compose up -d
```

---

## 🌐 Accessing Airflow

Once everything is up, go to:

👉 [http://localhost:8080](http://localhost:8080)

Login with:
```
Username: airflow2
Password: airflow2
```

You should see all five DAGs listed:
```
master_trigger_dag
dag_1_extract
dag_2_transform
dag_3_train
dag_4_deploy
```

---

## ▶️ How to run the pipeline

You can either:

### 🔹 Run it manually
1. Turn ON all DAGs (toggle switch to blue).  
2. Click ▶️ **Trigger DAG** next to `master_trigger_dag`.

It will automatically trigger the remaining four DAGs in sequence:
```
Extract → Transform → Train → Deploy
```

### 🔹 Or automate it (scheduled)
In `master_trigger_dag.py`, set:
```python
schedule_interval='@daily'
```
Now Airflow will automatically execute the entire workflow every day.

---

## 📦 Outputs

| Stage | Output Path | Description |
|--------|--------------|--------------|
| **Extract** | `working_data/raw/iris.csv` | Raw Iris dataset |
| **Transform** | `working_data/cleaned/iris_clean.csv` | Preprocessed dataset |
| **Train** | `working_data/model/iris_model.pkl` | Trained Logistic Regression model |
| **Deploy** | `working_data/model/predictions.json` | Predictions generated by the model |

---

## 👨‍💻 Author

**Ashish Gajjela**  
Master of Science in Artificial Intelligence  
**Northeastern University, Boston**    
📍 [LinkedIn](https://linkedin.com/in/ashishgajjela)

---

### ⭐ In short:
You just need to:
```bash
sudo docker compose up airflow-init
sudo docker compose up -d
```
Then open [http://localhost:8080](http://localhost:8080) and trigger `master_trigger_dag` —  
and watch the entire Iris ML pipeline run automatically 🚀
