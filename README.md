# ğŸš€ MLOps Lab 2 â€“ Building an Automated ML Pipeline using Apache Airflow (with Docker)

Hi! Iâ€™m **Ashish Gajjela**, and this is my submission for **MLOps Lab 2**, where I built a **fully automated machine learning pipeline** using **Apache Airflow** running inside **Docker**.

This project shows how to orchestrate an end-to-end ML workflow â€” from **data extraction** all the way to **model deployment** â€” using modular DAGs that trigger one another automatically.

---

## ğŸ§© What this project does

The pipeline is divided into **five DAGs**, each handling one stage of the ML lifecycle.  
Hereâ€™s how the data flows through them:

```
master_trigger_dag â†’ dag_1_extract â†’ dag_2_transform â†’ dag_3_train â†’ dag_4_deploy
```

### ğŸ§  Step-by-step explanation:

1. **dag_1_extract**  
   - Downloads the **Iris dataset** from a public source.  
   - Saves it in the `working_data/raw` folder as `iris.csv`.

2. **dag_2_transform**  
   - Reads the raw dataset, performs preprocessing (cleaning, encoding, scaling, etc.).  
   - Outputs a processed version to `working_data/processed/iris_clean.csv`.

3. **dag_3_train**  
   - Loads the processed data.  
   - Trains a **Logistic Regression** model using `scikit-learn`.  
   - Saves the trained model as `working_data/models/model.pkl`.

4. **dag_4_deploy**  
   - Simulates deployment by loading the model and running predictions.  
   - Exports results to `working_data/output/predictions.csv`.

5. **master_trigger_dag**  
   - Orchestrates everything.  
   - When triggered, it automatically runs all other DAGs in order.

---

## ğŸ§  What I used

- **Apache Airflow** (v2.5.1) with **CeleryExecutor**  
- **Docker Compose** (to containerize everything)
- **PostgreSQL** (metadata database)
- **Redis** (task broker)
- **Python 3.7+**
- **Libraries:** `pandas`, `scikit-learn`, `requests`, `joblib`

---

## ğŸ“ Project structure

```
MLOps-Lab2-Airflow/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_1_extract.py
â”‚   â”œâ”€â”€ dag_2_transform.py
â”‚   â”œâ”€â”€ dag_3_train.py
â”‚   â”œâ”€â”€ dag_4_deploy.py
â”‚   â””â”€â”€ master_trigger_dag.py
â”‚
â”œâ”€â”€ working_data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ cleanedd/
â”‚   â””â”€â”€ model/
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ logs/
â”œâ”€â”€ plugins/
â””â”€â”€ README.md
```

---

## âš™ï¸ How to set it up (Step-by-Step)

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/ashish159565/MLOps-Lab2-Airflow
cd MLOps-Lab2-Airflow
```

### 2ï¸âƒ£ Create the `.env` file
Inside your project folder, create a `.env` file with the following:

```bash
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=.
_AIRFLOW_WWW_USER_USERNAME=airflow2
_AIRFLOW_WWW_USER_PASSWORD=airflow2
```

### 3ï¸âƒ£ Set up permissions
Airflow runs inside the container as user ID 50000, so give it permission to write data:

```bash
sudo mkdir -p working_data/raw working_data/processed working_data/models
sudo chown -R 50000:0 working_data
sudo chmod -R 775 working_data
```

### 4ï¸âƒ£ Make sure DAGs auto-enable
Open `docker-compose.yaml` and change:
```yaml
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
```

This ensures all DAGs are active automatically.

### 5ï¸âƒ£ Initialize and start Airflow
Now build and run everything:

```bash
sudo docker compose down --volumes --remove-orphans
sudo docker compose up airflow-init
sudo docker compose up -d
```

---

## ğŸŒ Accessing Airflow

Once everything is up, go to:

ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Login with:
```
Username: airflow2
Password: airflow2
```

You should see all five DAGs listed:
```
master_trigger_dag
dag_1_extract
dag_2_transform
dag_3_train
dag_4_deploy
```

---

## â–¶ï¸ How to run the pipeline

You can either:

### ğŸ”¹ Run it manually
1. Turn ON all DAGs (toggle switch to blue).  
2. Click â–¶ï¸ **Trigger DAG** next to `master_trigger_dag`.

It will automatically trigger the remaining four DAGs in sequence:
```
Extract â†’ Transform â†’ Train â†’ Deploy
```

### ğŸ”¹ Or automate it (scheduled)
In `master_trigger_dag.py`, set:
```python
schedule_interval='@daily'
```
Now Airflow will automatically execute the entire workflow every day.

---

## ğŸ“¦ Outputs

| Stage | Output Path | Description |
|--------|--------------|--------------|
| **Extract** | `working_data/raw/iris.csv` | Raw Iris dataset |
| **Transform** | `working_data/cleaned/iris_clean.csv` | Preprocessed dataset |
| **Train** | `working_data/model/iris_model.pkl` | Trained Logistic Regression model |
| **Deploy** | `working_data/model/predictions.json` | Predictions generated by the model |

---

## ğŸ‘¨â€ğŸ’» Author

**Ashish Gajjela**  
Master of Science in Artificial Intelligence  
**Northeastern University, Boston**    
ğŸ“ [LinkedIn](https://linkedin.com/in/ashishgajjela)

---

### â­ In short:
You just need to:
```bash
sudo docker compose up airflow-init
sudo docker compose up -d
```
Then open [http://localhost:8080](http://localhost:8080) and trigger `master_trigger_dag` â€”  
and watch the entire Iris ML pipeline run automatically ğŸš€
